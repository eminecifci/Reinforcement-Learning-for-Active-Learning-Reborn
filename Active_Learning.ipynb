{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Data:  60000\n",
      "Number of Test Data:  10000\n",
      "Number of class:  10\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "ncolor = 1\n",
    "nclass = 10\n",
    "ntrain = len(x_train)\n",
    "ntest = len(x_test)\n",
    "\n",
    "x_train = x_train/255.0\n",
    "x_train = np.reshape(x_train, (ntrain, -1))\n",
    "x_test = x_test/255.0\n",
    "x_test = np.reshape(x_test, (ntest, -1))\n",
    "\n",
    "labels = np.zeros((ntrain, nclass))\n",
    "labels[np.arange(ntrain), y_train] = 1\n",
    "y_train = labels\n",
    "\n",
    "labels = np.zeros((ntest, nclass))\n",
    "labels[np.arange(ntest), y_test] = 1\n",
    "y_test = labels\n",
    "\n",
    "print(\"Number of Training Data: \", ntrain)\n",
    "print(\"Number of Test Data: \", ntest)\n",
    "print(\"Number of class: \", nclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38691\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADxBJREFUeJzt3X+QVfV5x/HPw7KAIDQg5TeKNZiotGBcsa22Y2q0Ws2g02JkOg06sZtabeM0k8YynYlp08YxGpupxskaScg0YmwNhUmhyjBxiE1qXRgaVMIP7arIyiZgRkgafixP/9iDXXHP917vPfeeuzzv1wyz957nnnMe7u5nz737ved8zd0FIJ4RZTcAoByEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUCObubNRNtrHaFwzdwmE8gv9TIf9kFXz2LrCb2ZXSvqSpDZJX3X3u1KPH6Nxusguq2eXABKe8Q1VP7bml/1m1ibpAUlXSTpX0hIzO7fW7QFornre8y+UtMvdX3L3w5IelbSomLYANFo94Z8p6dVB93dny97GzDrNrNvMuo/oUB27A1CkesI/1B8V3nF+sLt3uXuHu3e0a3QduwNQpHrCv1vS7EH3Z0naU187AJqlnvA/K2mumZ1pZqMk3SBpTTFtAWi0mof63P2omd0m6QkNDPUtd/fnC+sMQEPVNc7v7mslrS2oFwBNxMd7gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiqqVN0A800cuaM3Nobl5yeXHfCrgPJum8a/lep58gPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0HVNc5vZj2SDkjql3TU3TuKaAoxjBg/Plnf/rnzkvXl13Ql65Pa/jO3dk57e3LdO16/MFnfvCz9oz7qie5kvRUU8SGfD7r7TwrYDoAm4mU/EFS94XdJT5rZJjPrLKIhAM1R78v+i919j5lNkbTezH7k7hsHPyD7pdApSWM0ts7dAShKXUd+d9+Tfe2TtErSwiEe0+XuHe7e0a7R9ewOQIFqDr+ZjTOz8cdvS7pC0nNFNQagsep52T9V0iozO76dR9z93wvpCkDD1Rx+d39J0vwCe8EwVGmsfsTkSbm1uf/yWnLdNdMeqKmnt/atUbm1Y/LkundNezZZX9B5TrI+64lkuSUw1AcERfiBoAg/EBThB4Ii/EBQhB8Iikt3oy47Pps+7XbHR76cW6s03FbJygNTk/XPrvuD3Nqtlz+ZXPfPJu5M1u+b/1iyfq/Sz0sr4MgPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzh+cXZAej77xkbXJ+uJTN9e871eO/m+yfu0Df5msz7j7+8n6e5V/6e6e7tOS647QrmT98f2VrlKf/r+1Ao78QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4/wngZHT8s9rPzx3RnLd27+2Mlm/4pSfJevHklXpup3X5NYO3DM7ue6M76TH8evxb09fkKzfuzj/MwKS9PuT0lNwcz4/gJZF+IGgCD8QFOEHgiL8QFCEHwiK8ANBVRznN7Plkq6R1Ofu87JlkyR9S9IcST2Srnf3NxrXZmw2Mv1t+p+bz8qtbb3l/uS6la6dv7c/fV76b62/PVk/+2P54+Fj1Jtct5EuvHBHXetvOND64/iVVHPk/7qkK09YdoekDe4+V9KG7D6AYaRi+N19o6T9JyxeJGlFdnuFpGsL7gtAg9X6nn+qu/dKUvZ1SnEtAWiGhn+238w6JXVK0hiNbfTuAFSp1iP/XjObLknZ1768B7p7l7t3uHtHu0bXuDsARas1/GskLc1uL5W0uph2ADRLxfCb2UpJP5D0PjPbbWYfk3SXpMvNbKeky7P7AIaRiu/53X1JTumygnsJK3U+viS98DenJ+s7rv7HRNWS6z5/+Giyfv2jn0rWz73/5WQ9vfXyvHrgPXWtf94pu5P1LZpV1/abgU/4AUERfiAowg8ERfiBoAg/EBThB4Li0t1NMHJm+vLZPR+dk6zvujp9Wm5qOG/r4SPJNW/++/QpuWc+9INkvVWH8qT0895/LH3cG1FhiPT+Fz+YrE/UzmS9FXDkB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOdvgn0PpS9ftmV+6pTcytNgp1Qaxz+twjj+cHZkTv6lJVfNS392ore/wsYfm1zhAYzzA2hRhB8IivADQRF+ICjCDwRF+IGgCD8QFOP8VWp7zy/l1nb89TnJdbfP/3KyXmkcf/fR9DTZX+j7UG7tZB7Hr+Sn7z0ltza5Lb8mSet+Pj5Zn/yd7cl6pY8JtAKO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVMVxfjNbLukaSX3uPi9bdqekP5b04+xhy9x9baOabIZK02Tv+crE3Nq2jgcqbD19DfhKfmfdXyTrZ//Jf9W1/eFqxK+9P1k//9YtNW/787uuStYn7Hux5m23imqO/F+XdOUQy+9z9wXZv2EdfCCiiuF3942S9jehFwBNVM97/tvM7IdmttzM8l8TA2hJtYb/QUlnSVogqVfSvXkPNLNOM+s2s+4jOlTj7gAUrabwu/ted+9392OSHpK0MPHYLnfvcPeOdo2utU8ABasp/GY2fdDd6yQ9V0w7AJqlmqG+lZIulTTZzHZL+oykS81sgSSX1CPp4w3sEUADVAy/uy8ZYvHDDeilVNv+6sxkfWdH6pz8+sbxf/U/bkzWo47jt533vmR92y0TkvV1s/KvZfDikfQ1Eg5+N/25jwmKMc4P4CRE+IGgCD8QFOEHgiL8QFCEHwgqzKW7D/9uR7L+1HX3JOvHlL7Uc0qlobwzbnih5m0PZ/4b85P1g3e+mazvmPdgst7v+UOwS7d9NLnujLu/n6yfDDjyA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQYcb5z/7b55P16RWmbE5ue/Ut6fqfxjwlV0qflvubX3kmue6nT0t/zyq5evuHc2sT/vCnyXWHwxTb9eLIDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBhRnnr9fe/vxLPc/9p7jTkB1cfFGy3n/TvtxaveP4H3lxqMmj/1/bze25taP7Xqtr3ycDjvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFTFcX4zmy3pG5KmSTomqcvdv2RmkyR9S9IcST2Srnf3NxrXamONqDDN9vS2sbm1df/8teS6T/0if7xZkj7x3zck6/Uw82TdE9e2l6RHPpCejf289s3JepvlH19S19WX0ufjS+lxfEk6+lJPsh5dNUf+o5I+6e7nSPp1Sbea2bmS7pC0wd3nStqQ3QcwTFQMv7v3uvvm7PYBSdskzZS0SNKK7GErJF3bqCYBFO9dvec3szmSzpf0jKSp7t4rDfyCkDSl6OYANE7V4TezUyU9Lul2d09Povb29TrNrNvMuo8o7mfggVZTVfjNrF0Dwf+mu387W7zXzKZn9emS+oZa19273L3D3TvaNbqIngEUoGL4zcwkPSxpm7t/cVBpjaSl2e2lklYX3x6ARjH39FCQmV0i6XuStmpgqE+Slmngff9jkk6X9Iqkxe6+P7WtCTbJL7LL6u25Jn2r35+sP3VBerhurI2qed+VhhGPKf09qEeZ+5akT72ef8rvps9dkFx3/MadyXr/vuSPW0jP+Aa96fvT3/RMxXF+d39ayv0JKifJAOrGJ/yAoAg/EBThB4Ii/EBQhB8IivADQYW5dPeURT9K1hdd/efJ+ssfzh86nXZG/uWpJenzZ69K1m/aeFOyrkNt6XrC6L3pb/Hs9T+vedvVGLWrN7c29vX0FN0RpskuE0d+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiq4vn8RSrzfH4ggndzPj9HfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiqYvjNbLaZfdfMtpnZ82b2iWz5nWb2mpltyf79XuPbBVCUaibtOCrpk+6+2czGS9pkZuuz2n3ufk/j2gPQKBXD7+69knqz2wfMbJukmY1uDEBjvav3/GY2R9L5ko7Ps3Sbmf3QzJab2cScdTrNrNvMuo/oUF3NAihO1eE3s1MlPS7pdnd/U9KDks6StEADrwzuHWo9d+9y9w5372jX6AJaBlCEqsJvZu0aCP433f3bkuTue929392PSXpI0sLGtQmgaNX8td8kPSxpm7t/cdDy6YMedp2k54pvD0CjVPPX/osl/ZGkrWa2JVu2TNISM1sgySX1SPp4QzoE0BDV/LX/aUlDXQd8bfHtAGgWPuEHBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8Iyty9eTsz+7GklwctmizpJ01r4N1p1d5atS+J3mpVZG9nuPsvV/PApob/HTs363b3jtIaSGjV3lq1L4nealVWb7zsB4Ii/EBQZYe/q+T9p7Rqb63al0RvtSqlt1Lf8wMoT9lHfgAlKSX8ZnalmW03s11mdkcZPeQxsx4z25rNPNxdci/LzazPzJ4btGySma03s53Z1yGnSSupt5aYuTkxs3Spz12rzXjd9Jf9ZtYmaYekyyXtlvSspCXu/kJTG8lhZj2SOty99DFhM/ttSQclfcPd52XL7pa0393vyn5xTnT3T7dIb3dKOlj2zM3ZhDLTB88sLelaSTeqxOcu0df1KuF5K+PIv1DSLnd/yd0PS3pU0qIS+mh57r5R0v4TFi+StCK7vUIDPzxNl9NbS3D3XnffnN0+IOn4zNKlPneJvkpRRvhnSnp10P3daq0pv13Sk2a2ycw6y25mCFOzadOPT58+peR+TlRx5uZmOmFm6ZZ57mqZ8bpoZYR/qNl/WmnI4WJ3/4CkqyTdmr28RXWqmrm5WYaYWbol1DrjddHKCP9uSbMH3Z8laU8JfQzJ3fdkX/skrVLrzT689/gkqdnXvpL7eUsrzdw81MzSaoHnrpVmvC4j/M9KmmtmZ5rZKEk3SFpTQh/vYGbjsj/EyMzGSbpCrTf78BpJS7PbSyWtLrGXt2mVmZvzZpZWyc9dq814XcqHfLKhjH+Q1CZpubv/XdObGIKZ/YoGjvbSwCSmj5TZm5mtlHSpBs762ivpM5L+VdJjkk6X9Iqkxe7e9D+85fR2qQZeur41c/Px99hN7u0SSd+TtFXSsWzxMg28vy7tuUv0tUQlPG98wg8Iik/4AUERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8I6v8ARI4jSleNZn8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.random.randint(ntrain)\n",
    "\n",
    "imgplot = plt.imshow(np.reshape(x_train[idx], (28,28)))\n",
    "print(idx)\n",
    "print(y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2383\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.random.randint(ntest)\n",
    "\n",
    "imgplot1 = plt.imshow(np.reshape(x_test[idx], (28,28)))\n",
    "print(idx)\n",
    "print(y_test[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_model(object):\n",
    "    def __init__(self, sess, img_width, img_height, n_class, lr=1e-4):\n",
    "        \n",
    "        n_features = img_width * img_height\n",
    "        self.sess = sess\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [None, 28*28])\n",
    "        image = tf.reshape(self.x, [-1, 28, 28, 1])              # (batch, height, width, channel)\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, 10])            # input y\n",
    "        self.dropout_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "        with tf.variable_scope('cnn_model'):\n",
    "            \n",
    "            # Convolutional layer 1\n",
    "            conv1 = tf.layers.conv2d(\n",
    "                inputs=image,\n",
    "                filters=32,\n",
    "                kernel_size=5,\n",
    "                strides=1,\n",
    "                padding='same',\n",
    "                activation=tf.nn.relu\n",
    "            )\n",
    "            pool1 = tf.layers.max_pooling2d(conv1, pool_size=2, strides=2,)\n",
    "\n",
    "            # Convolutional layer 2\n",
    "            conv2 = tf.layers.conv2d(pool1, 64, 5, 1, 'same', activation=tf.nn.relu)\n",
    "            pool2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "            pool2_flat = tf.reshape(pool2, [-1, 7*7*64])\n",
    "\n",
    "            # Fully connected layer 1\n",
    "            fc1 = tf.layers.dense(\n",
    "                inputs=pool2_flat,\n",
    "                units=1024,    # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),    # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='Dense1'\n",
    "            )\n",
    "\n",
    "            # Dropout\n",
    "            d1 = tf.layers.dropout(inputs=fc1, rate=self.dropout_rate)\n",
    "\n",
    "            # Fully connected layer 2\n",
    "            self.output = tf.layers.dense(d1, 10)\n",
    "            self.y = tf.contrib.layers.softmax(self.output)\n",
    "\n",
    "\n",
    "            self.loss = tf.reduce_mean(-tf.reduce_sum(self.y_ * tf.log(self.y), reduction_indices=[1]))\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "            self.train_op = tf.train.AdamOptimizer(1e-4).minimize(self.loss)\n",
    "            \n",
    "#             self.loss = tf.losses.softmax_cross_entropy(onehot_labels=self.y_, logits=output)\n",
    "#             self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "#             self.accuracy = tf.metrics.accuracy(\n",
    "#                 labels=tf.argmax(self.y_, axis=1), \n",
    "#                 predictions=tf.argmax(output, axis=1),)[1]\n",
    "            \n",
    "            \n",
    "    def train(self, x, y_):\n",
    "        _, loss = self.sess.run([self.train_op, self.loss], {self.x:x, self.y_:y_, self.dropout_rate:0.5})\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, x, dropout_rate=0.0):\n",
    "        output = self.sess.run(self.y, {self.x:x, self.dropout_rate:dropout_rate})\n",
    "        pred_y = np.argmax(output, 1)\n",
    "        return [output, pred_y]\n",
    "    \n",
    "    def evaluate(self, x, y_):\n",
    "        acc, loss = self.sess.run([self.accuracy, self.loss], {self.x:x, self.y_:y_, self.dropout_rate:0.0})\n",
    "        return [acc, loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.Session()\n",
    "# cnn = cnn_model(sess, 28, 28, nclass, 0.001)\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# batch_size = 1024\n",
    "\n",
    "# for step in range(600):\n",
    "#     start = step * batch_size\n",
    "#     end = min((step+1) * batch_size, ntrain)    \n",
    "#     training_data = x_train[start: end]\n",
    "#     training_labels = y_train[start: end]\n",
    "\n",
    "#     cnn.train(training_data, training_labels)\n",
    "\n",
    "# print(\"Training End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######Ouptut layer size determined by labeling function\n",
    "# output_layer_size = nclass\n",
    "\n",
    "# ######Here is the neural net model described in Tensor Flow MNIST example\n",
    "# def weight_variable(shape):\n",
    "#   initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "#   return tf.Variable(initial)\n",
    "\n",
    "# def bias_variable(shape):\n",
    "#   initial = tf.constant(0.1, shape=shape)\n",
    "#   return tf.Variable(initial)\n",
    "\n",
    "# def conv2d(x, W):\n",
    "#   return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# def max_pool_2x2(x):\n",
    "#   return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "#                         strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# x  = tf.placeholder(tf.float32, [None, 784], name='x')\n",
    "# x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "# y_ = tf.placeholder(tf.float32, [None, output_layer_size],  name='y_')\n",
    "\n",
    "# # Convolutional layer 1\n",
    "# W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "# b_conv1 = bias_variable([32])\n",
    "# h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "# h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# # Convolutional layer 2\n",
    "# W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "# b_conv2 = bias_variable([64])\n",
    "# h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "# h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# # Fully connected layer 1\n",
    "# h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "# W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "# b_fc1 = bias_variable([1024])\n",
    "# h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# # Dropout\n",
    "# keep_prob  = tf.placeholder(tf.float32)\n",
    "# h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# # Fully connected layer 2 (Output layer)\n",
    "# W_fc2 = weight_variable([1024, output_layer_size])\n",
    "# b_fc2 = bias_variable([output_layer_size])\n",
    "\n",
    "# y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2, name='y')\n",
    "\n",
    "# # Evaluation functions\n",
    "# cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "\n",
    "# # Training algorithm\n",
    "# train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ranker_random(examples, cnn, sess=None):\n",
    "    return np.random.randint(10, size=len(examples))\n",
    "\n",
    "def ranker_max_min(examples, cnn, sess):\n",
    "    s, pred_y = cnn.predict(examples)\n",
    "    ranked = np.max(s, axis=1) - np.min(s, axis=1)\n",
    "    return ranked\n",
    "\n",
    "def ranker_best_second(examples, cnn, sess):\n",
    "    s, pred_y = cnn.predict(examples)\n",
    "    s = np.sort(s, axis=1)\n",
    "    ranked = s[-1] - s[-2]\n",
    "    return ranked\n",
    "\n",
    "# def ranker_bayesian(examples, cnn, sess):\n",
    "#     predicts = np.asarray([sess.run(y, feed_dict={x: examples, keep_prob: 0.5}) for i in range(15)])\n",
    "#     var = np.var(predicts, axis=0)\n",
    "#     var_sum = np.sum(var, axis=1)\n",
    "#     return var_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: rename batch_size, return np array instead of list\n",
    "\n",
    "###### Ranks mnist training images according to a rank function (random for normal, \n",
    "###### model evaluation for active learning)\n",
    "def choose_examples(cnn, datas = x_train, remain = [], batch_size = 50, ranker = ranker_random, chosen = [], sess = None):\n",
    "\n",
    "    #do not look at examples that have already been seen\n",
    "    lookup_index = range(0, ntrain)\n",
    "    look_size = min(batch_size * 30, len(remain))\n",
    "    random_draw = random.sample(range(len(remain)), look_size)\n",
    "\n",
    "    #rank the examples according to the rank function\n",
    "    remain_idx = remain[random_draw]\n",
    "    remain_data = datas[remain_idx]\n",
    "    remain = np.delete(remain, idx)\n",
    "    ranks = ranker(remain_data, cnn, sess)\n",
    "    scores = np.column_stack((remain_idx, ranks))\n",
    "    sort = scores[np.argsort(scores[:,1])]\n",
    "\n",
    "    #select examples based on their scores. we only pick the first {batch_size}th of data\n",
    "    batch_size = min(batch_size, len(scores))\n",
    "    selected = sort[:batch_size, 0]\n",
    "    return [selected.astype(int).tolist(), remain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "######trains one model on a subset of mnist data for a certian number of epochs\n",
    "def epoch_sample(cnn, chosen, batch_size, epochs, sess):\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    runs = int(len(chosen) / batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(chosen)\n",
    "        \n",
    "        for i in range(runs):\n",
    "            end = min((i + 1) * batch_size, len(chosen)-1)\n",
    "            batch_idx = chosen[i * batch_size: end]\n",
    "            training_data = x_train[batch_idx]\n",
    "            training_labels = y_train[batch_idx]\n",
    "#             sess.run(train_step, feed_dict={x: training_data, y_: training_labels, keep_prob: 0.5})\n",
    "            cnn.train(training_data, training_labels)\n",
    "\n",
    "#     [epoch_acc, epoch_ce] = sess.run([accuracy, cross_entropy], feed_dict={x: x_test, y_: y_test, keep_prob: 1.0})\n",
    "    [acc, ce] = cnn.evaluate(x_test, y_test)\n",
    "    print(\"Labels: \", len(chosen), \" Epochs: \", epochs, \" Acc: \", acc, \" Cross Entropy: \", ce)\n",
    "    \n",
    "    return [acc, ce]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_batch(size, max_steps, ranker, print_every):\n",
    "    #run_log = []\n",
    "    #batch_log = []\n",
    "    chosen = []\n",
    "    remain = np.arange(ntrain)\n",
    "\n",
    "    sess = tf.Session()\n",
    "        \n",
    "    cnn = cnn_model(sess, 28, 28, 10, 0.001)\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) # the local var is for accuracy_op\n",
    "    sess.run(init_op)\n",
    "\n",
    "    print(\"Select Active Learning Dataset\")\n",
    "    timer1 = 0\n",
    "    timer2 = 0\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        start_time = time.time()\n",
    "        #if re-sampling turned on, will double size of the mini-batch\n",
    "        #by randomly sampling from previously trained on exmaples\n",
    "        next_batch, remain = choose_examples(cnn, x_train, remain, size, ranker, chosen, sess)\n",
    "        chosen = chosen + next_batch\n",
    "        if len(remain) <= 0:\n",
    "            remain = np.arange(ntrain)\n",
    "\n",
    "        batch_xs = x_train[next_batch]\n",
    "        batch_ys = y_train[next_batch]\n",
    "        end_time = time.time()\n",
    "        timer1 = timer1 + end_time - start_time\n",
    "\n",
    "        #Train the model using the mini-batch\n",
    "        start_time = time.time()\n",
    "#             sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "        cnn.train(batch_xs, batch_ys)\n",
    "\n",
    "        if (step % print_every) == 0:\n",
    "#                 [acc, ce] = sess.run([accuracy, cross_entropy], feed_dict={x: x_test, y_: y_test, keep_prob: 1.0})\n",
    "            [acc, ce] = cnn.evaluate(x_test, y_test)\n",
    "#                 print(acc)\n",
    "#                 run_log.append([step, acc, ce])\n",
    "        end_time = time.time()\n",
    "        timer2 = timer2 + end_time - start_time\n",
    "\n",
    "    print(timer1)\n",
    "    print(timer2)\n",
    "#         [final, final_ce] = sess.run([accuracy, cross_entropy], feed_dict={x: x_test, y_: y_test, keep_prob: 1.0})\n",
    "    [final, final_ce] = cnn.evaluate(x_test, y_test)\n",
    "    #run_log.append([max_steps, final, final_ce])\n",
    "    #batch_log.append(run_log)\n",
    "\n",
    "    #start multi-epoch portion (kind of pasted on at the end)\n",
    "    epoch_logs = []\n",
    "    label_range = 250\n",
    "    epochs = 20\n",
    "    epoch_batch_size = 50\n",
    "    iteration = 20\n",
    "\n",
    "    print(\"\\nStart Training the Network with the selected Dataset\")\n",
    "    start_time = time.time()\n",
    "    #for label_size in range(iteration):\n",
    "    labels_length = (iteration + 1) * label_range\n",
    "    result = epoch_sample(cnn, chosen[0: labels_length], epoch_batch_size, epochs, sess)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    #epoch_logs.append([labels_length, epochs, result[0], result[1]])\n",
    "\n",
    "#         print(\"labels\\tepoch\\taccuracy\\tcross entropy\")\n",
    "#         for entry in epoch_logs:\n",
    "#             print(entry[0], \"\\t\", entry[1], \"\\t\", entry[2], \"\\t\", entry[3])\n",
    "    print(\"\\nFinish Training\")\n",
    "    #return batch_log\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def print_average_series(to_average, columns, column_names):\n",
    "#    labels = [int(a[0]) for a in to_average[0]]\n",
    "#   column_collection = []\n",
    "#   for column in columns:\n",
    "#         transformed = []\n",
    "#         for run in to_average:\n",
    "#             transformed.append([a[column] for a in run])\n",
    "#         column_collection.append(np.mean(np.transpose(transformed), axis=1))\n",
    "#     column_collection.insert(0, np.array(labels))\n",
    "#     column_names.insert(0, 'iteration')\n",
    "#     print(*column_names, sep='\\t')\n",
    "#     for row in np.transpose(column_collection):\n",
    "#         print(*row, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Active Learning Dataset\n",
      "10.942887783050537\n",
      "8.227154731750488\n",
      "\n",
      "Start Training the Network with the selected Dataset\n",
      "Labels:  5250  Epochs:  20  Acc:  0.9885  Cross Entropy:  0.03518779\n",
      "16.50696110725403\n",
      "\n",
      "Finish Training\n",
      "37.06350302696228\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "iterations = 1000\n",
    "print_every = 100\n",
    "ranker = ranker_max_min\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "results = run_batch(batch_size, iterations, ranker, print_every)\n",
    "#pint_average_series(results, [1, 2], [\"accuracy\", \"cross entropy\"])\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
