{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Data:  60000\n",
      "Number of Test Data:  10000\n",
      "Number of class:  10\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "ncolor = 1\n",
    "nclass = 10\n",
    "width  = 28\n",
    "height = 28\n",
    "ntrain = len(x_train)\n",
    "ntest  = len(x_test)\n",
    "\n",
    "x_train = x_train/255.0\n",
    "x_train = np.reshape(x_train, (ntrain, -1))\n",
    "x_test = x_test/255.0\n",
    "x_test = np.reshape(x_test, (ntest, -1))\n",
    "\n",
    "labels = np.zeros((ntrain, nclass))\n",
    "labels[np.arange(ntrain), y_train] = 1\n",
    "y_train = labels\n",
    "\n",
    "labels = np.zeros((ntest, nclass))\n",
    "labels[np.arange(ntest), y_test] = 1\n",
    "y_test = labels\n",
    "\n",
    "print(\"Number of Training Data: \", ntrain)\n",
    "print(\"Number of Test Data: \", ntest)\n",
    "print(\"Number of class: \", nclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classification_model(object):\n",
    "    def __init__(self, img_w, img_h, n_class, lr=1e-4):\n",
    "\n",
    "        # Placeholder Input\n",
    "        x = tf.placeholder(tf.float32, [None, img_w*img_h])           # (batch, height, width, channel)\n",
    "        y_ = tf.placeholder(tf.float32, [None, n_class])            # input y\n",
    "        img = tf.reshape(x, [-1, img_w, img_h, 1])\n",
    "        dropout_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "        # Network\n",
    "        conv1 = tf.layers.conv2d(img, 32, 5, 1, 'same', activation=tf.nn.relu)\n",
    "        pool1 = tf.layers.max_pooling2d(conv1, pool_size=2, strides=2,)\n",
    "        conv2 = tf.layers.conv2d(pool1, 64, 5, 1, 'same', activation=tf.nn.relu)\n",
    "        pool2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "        pool2f = tf.reshape(pool2, [-1, 7*7*64])\n",
    "        fc1 = tf.layers.dense(pool2f, 1024, tf.nn.relu)\n",
    "        drop1 = tf.layers.dropout(inputs=fc1, rate=dropout_rate)\n",
    "        output = tf.layers.dense(drop1, n_class)\n",
    "        y = tf.contrib.layers.softmax(output)\n",
    "\n",
    "        # Optimizer\n",
    "        loss = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "        train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "        # Variables\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.y_ = y_\n",
    "        self.loss = loss\n",
    "        self.accuracy = accuracy\n",
    "        self.train_op = train_op\n",
    "        self.dropout_rate = dropout_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def classification(data, epochs, batch_size, img_w, img_h, n_class, dropout=0.5, lr=1e-4):     \n",
    "    x_train, y_train, x_test, y_test = data\n",
    "\n",
    "    # Create a model first\n",
    "    cnn = classification_model(img_w, img_h, n_class)\n",
    "    runs = int(len(x_train) / batch_size)\n",
    "    train_order = np.arange(len(x_train))\n",
    "\n",
    "    # Training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        print(\"Training Iteration Started: \")\n",
    "        bgn_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            random.shuffle(train_order)\n",
    "\n",
    "            for i in range(runs):\n",
    "                bgn = i * batch_size\n",
    "                end = min((bgn + batch_size, len(x_train)-1))\n",
    "                idx = train_order[bgn:end]\n",
    "\n",
    "                sess.run(cnn.train_op, {cnn.x:x_train[idx], cnn.y_:y_train[idx], cnn.dropout_rate:dropout})\n",
    "                if (i%250 == 0) or (i == runs-1):\n",
    "                    acc = sess.run(cnn.accuracy, {cnn.x:x_test, cnn.y_:y_test, cnn.dropout_rate:0.0})\n",
    "                    print(\"\\r  epoch:{0} iters:{1}/{2} acc:{3}        \".format(epoch, i, runs, acc))\n",
    "\n",
    "        acc, loss = sess.run([cnn.accuracy, cnn.loss], {cnn.x:x_test, cnn.y_:y_test, cnn.dropout_rate:0.0})\n",
    "        print(\"\\r  Labels: \", len(x_train), \" Epochs: \", epochs, \" Acc: \", acc, \" Cross Entropy: \", loss)\n",
    "        print(\"  Time Lapse: \", time.time() - bgn_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranker_random(batch, cnn, sess=None):\n",
    "    return np.random.randint(10, size=len(batch))\n",
    "\n",
    "def ranker_max_min(batch, cnn, sess):\n",
    "    pred_y = sess.run(cnn.y, {cnn.x:batch, cnn.dropout_rate:0.0})\n",
    "    ranked = np.max(pred_y, axis=1) - np.min(pred_y, axis=1)\n",
    "    return ranked\n",
    "\n",
    "def select_active_batch(cnn, x_train, lookup_list, counter, batch_size, sess, ranker):\n",
    "    reset = 0\n",
    "    if counter * 100 > len(x_train)-1:\n",
    "        reset = 1\n",
    "        lookup_list = random.shuffle(lookup_list)\n",
    "        counter = 0\n",
    "\n",
    "    bgn = counter * 100\n",
    "    end = min(bgn + 100, len(x_train)-1)\n",
    "    batch_idx = lookup_list[bgn:end]\n",
    "    batch = x_train[batch_idx]\n",
    "    counter = counter + 1\n",
    "\n",
    "    #get the network scores of selected data\n",
    "    ranked        = ranker(batch, cnn, sess)\n",
    "    ranked_scores = np.column_stack((batch_idx, ranked))\n",
    "    sorted_scores = ranked_scores[np.argsort(ranked_scores[:,1])]\n",
    "\n",
    "    #select examples based on their scores. we only pick the first {batch_size}th of data\n",
    "    return_size = min(batch_size, len(sorted_scores))\n",
    "    selected = sorted_scores[:return_size, 0]\n",
    "    return [selected.astype(int).tolist(), lookup_list, counter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_active_data(data, iters, batch_size, img_w, img_h, n_class, ranker, dropout=0.5, lr=1e-4):\n",
    "    x_train, y_train, x_test, y_test = data    \n",
    "\n",
    "    counter = 0\n",
    "    chosen = []\n",
    "    lookup_list = np.arange(len(x_train))\n",
    "    cnn = classification_model(img_w, img_h, n_class)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for i in range(iters):\n",
    "            batch_idx, lookup_list, counter = select_active_batch(cnn, x_train, lookup_list, counter, batch_size, sess, ranker)\n",
    "            sess.run(cnn.train_op, {cnn.x:x_train[batch_idx], cnn.y_:y_train[batch_idx], cnn.dropout_rate:dropout})\n",
    "            chosen = chosen + batch_idx\n",
    "\n",
    "            if (i%10 == 0) or (i == iters-1):\n",
    "                print(\"\\r  Iters:{0}/{1}       \".format(i, iters), end=\"\")\n",
    "\n",
    "        acc, loss = sess.run([cnn.accuracy, cnn.loss], {cnn.x:x_test, cnn.y_:y_test, cnn.dropout_rate:0.0})\n",
    "        print(\"\\n  Labels: \", len(x_train), \" Iters: \", iters, \" Acc: \", acc, \" Cross Entropy: \", loss)\n",
    "        \n",
    "    return chosen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Normal Learning Dataset: \n",
      "  Iters:499/500       \n",
      "  Labels:  60000  Iters:  500  Acc:  0.9287  Cross Entropy:  0.24258901\n",
      "Training Iteration Started: \n",
      "  epoch:0 iters:0/19 acc:0.20409999787807465        \n",
      "  epoch:0 iters:18/19 acc:0.7117000222206116        \n",
      "  epoch:1 iters:0/19 acc:0.7242000102996826        \n",
      "  epoch:1 iters:18/19 acc:0.7922999858856201        \n",
      "  epoch:2 iters:0/19 acc:0.7856000065803528        \n",
      "  epoch:2 iters:18/19 acc:0.8596000075340271        \n",
      "  epoch:3 iters:0/19 acc:0.8600000143051147        \n",
      "  epoch:3 iters:18/19 acc:0.8889999985694885        \n",
      "  epoch:4 iters:0/19 acc:0.8910999894142151        \n",
      "  epoch:4 iters:18/19 acc:0.9013000130653381        \n",
      "  Labels:  5000  Epochs:  5  Acc:  0.9013  Cross Entropy:  0.3610995\n",
      "  Time Lapse:  27.712556838989258\n",
      "\n",
      "Select Active Learning Dataset: \n",
      "  Iters:499/500       \n",
      "  Labels:  60000  Iters:  500  Acc:  0.9632  Cross Entropy:  0.15527913\n",
      "Training Iteration Started: \n",
      "  epoch:0 iters:0/19 acc:0.1111999973654747        \n",
      "  epoch:0 iters:18/19 acc:0.3131999969482422        \n",
      "  epoch:1 iters:0/19 acc:0.34209999442100525        \n",
      "  epoch:1 iters:18/19 acc:0.7182999849319458        \n",
      "  epoch:2 iters:0/19 acc:0.7469000220298767        \n",
      "  epoch:2 iters:18/19 acc:0.8669999837875366        \n",
      "  epoch:3 iters:0/19 acc:0.8720999956130981        \n",
      "  epoch:3 iters:18/19 acc:0.9067000150680542        \n",
      "  epoch:4 iters:0/19 acc:0.9028000235557556        \n",
      "  epoch:4 iters:18/19 acc:0.9312999844551086        \n",
      "  Labels:  5000  Epochs:  5  Acc:  0.9313  Cross Entropy:  0.3386682\n",
      "  Time Lapse:  27.63926410675049\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "dropout = 0.5\n",
    "\n",
    "iters       = 500\n",
    "select_size = 10\n",
    "train_epochs        = 5\n",
    "train_batch_size    = 256\n",
    "data = [x_train, y_train, x_test, y_test]\n",
    "\n",
    "# Normal Learning\n",
    "print(\"Select Normal Learning Dataset: \")\n",
    "normal = select_active_data(data, iters, select_size, width, height, nclass, ranker_random, dropout, lr)\n",
    "normal_data = [x_train[normal], y_train[normal], x_test, y_test]\n",
    "classification(normal_data, train_epochs, train_batch_size, width, height, nclass, dropout, lr)\n",
    "\n",
    "# Active Learning\n",
    "print(\"\\nSelect Active Learning Dataset: \")\n",
    "active = select_active_data(data, iters, select_size, width, height, nclass, ranker_max_min, dropout, lr)\n",
    "active_data = [x_train[active], y_train[active], x_test, y_test]\n",
    "classification(active_data, train_epochs, train_batch_size, width, height, nclass, dropout, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
